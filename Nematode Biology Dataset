import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
import string
from matplotlib import pyplot as plt
from collections import Counter
import numpy as np


nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

rx_dict = {
    'keys': re.compile(r'Key: (?P<keys>.*)\n'),
    'medline': re.compile(r'Medline: (?P<medline>\d+)\n'),
    'authors': re.compile(r'Authors: (?P<authors>.*)\n'),
    'title': re.compile(r'Title: (?P<title>.*)'),
    'citation': re.compile(r'Citation: (?P<citation>.*)'),
    'type': re.compile(r'Type: (?P<type>.*)'),
    'genes': re.compile(r'Genes: (?P<genes>.*)'),
    'abstract': re.compile(r'Abstract: (?P<abstract>.*)'),
    'end':re.compile(r'-------------------')
    }

def _findregex(line):
  for key,rx in rx_dict.items():
    match=rx.search(line)
    if match:
      return key,match
  return None,None

def parse_file(filepath):
  data=[]
  keyval='-'
  medval='-'
  author_val='-'
  title_val='-'
  citation_val='-'
  type_val='-'
  genes_val='-'
  abstract_val='-'

  with open(filepath,'r') as file_object:
    line=file_object.readline()
    while line:
      key,match=_findregex(line)
      if key =='keys':
        keyval=match.group('keys')
      if key =='medline':
        medval=match.group('medline')
      if key =='authors':
        author_val=match.group('authors')
      if key =='title':
        title_val=match.group('title')
      if key =='citation':
        citation_val=match.group('citation')
      if key =='type':
        type_val=match.group('type')
      if key =='genes':
        genes_val=match.group('genes')
      if key =='abstract':
        abstract_val=match.group('abstract')
      if key=='end':
        row = {
          'Key': keyval,
          'Medline': medval,
          'Authors': author_val,
          'Title': title_val,
          'Citation': citation_val,
          'Type': type_val,
          'Genes': genes_val,
          'Abstract': abstract_val ,
          }
        data.append(row)
      line=file_object.readline()
    data=pd.DataFrame(data)
    return data

Finaldata=parse_file('dta.csv')
print(Finaldata.head(1))

def remove_punct(text):
  return text.translate(str.maketrans('', '', string.punctuation))
def remove_stopword(text):
  STOPWORD = set(stopwords.words('english'))
  return " ".join([word for word in str(text).split() if word not in STOPWORD])
lemmatise = WordNetLemmatizer()
def lemmatize_word(text):
    return " ".join([lemmatise.lemmatize(word) for word in text.split()])
ps = PorterStemmer()
def stem_doc(text):
    return " ".join([ps.stem(word) for word in text.split()])

Finaldata["Title_cleaned"]=Finaldata["Title"].str.lower()
Finaldata["Title_cleaned"]=Finaldata["Title_cleaned"].apply(lambda text:remove_punct(text))
Finaldata["Title_cleaned"]=Finaldata["Title_cleaned"].apply(lambda text:remove_stopword(text))
Finaldata["Title_cleaned"]=Finaldata["Title_cleaned"].apply(lambda text:lemmatize_word(text))
Finaldata["Title_cleaned"]=Finaldata["Title_cleaned"].apply(lambda text:stem_doc(text))
print(Finaldata.head())

Finaldata["Abstract_cleaned"]=Finaldata["Abstract"].str.lower()
Finaldata["Abstract_cleaned"]=Finaldata["Abstract_cleaned"].apply(lambda text:remove_punct(text))
Finaldata["Abstract_cleaned"]=Finaldata["Abstract_cleaned"].apply(lambda text:remove_stopword(text))
Finaldata["Abstract_cleaned"]=Finaldata["Abstract_cleaned"].apply(lambda text:lemmatize_word(text))
Finaldata["Abstract_cleaned"]=Finaldata["Abstract_cleaned"].apply(lambda text:stem_doc(text))
print(Finaldata.head())

Finaldata["FinalText"]=Finaldata["Title_cleaned"]+" "+Finaldata["Abstract_cleaned"]
Finaldata["FinalText"]=Finaldata["FinalText"].apply(lambda text:nltk.word_tokenize(text))
print(Finaldata["FinalText"])


processed_docs={}
i=0
for row in Finaldata["FinalText"]:
  dictionary={}
  for word in row:
    if word not in processed_docs.keys():
      dictionary[word]=1
    else:
      dictionary[word]=1+dictionary[word]
  processed_docs[i]=dictionary
  i=i+1

total_word_count={}
for row in Finaldata["FinalText"]:
  for word in row:
    if word not in total_word_count.keys():
      total_word_count[word]=1
    else:
      total_word_count[word]+=1
print(total_word_count)

'''plt.figure(2, figsize=(15, 15/1.6180))
counts = Counter(total_word_count)
labels, values = zip(*counts.items())
indSort = np.argsort(values)[::-1]
labels = np.array(labels)[indSort]
values = np.array(values)[indSort]
indexes = np.arange(len(labels))
bar_width = 0.35
plt.bar(indexes, values)
plt.xticks(indexes + bar_width, labels)
plt.show()'''


from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA
#count_vectorizer = CountVectorizer(stop_words='english')
#count_data = count_vectorizer.fit_transform(Finaldata["Abstract_cleaned"])
#number_topics = 5
#lda = LDA(n_components=number_topics)
#lda.fit(count_data)
#print("Topics found via LDA:")
#print_topics(lda, count_vectorizer, number_topics)

from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style('whitegrid')

# Helper function
def plot_10_most_common_words(count_data, count_vectorizer):
  import matplotlib.pyplot as plt
  words = count_vectorizer.get_feature_names()
  total_counts = np.zeros(len(words))
  for t in count_data:
    total_counts += t.toarray()[0]

  count_dict = (zip(words, total_counts))
  count_dict = sorted(count_dict, key=lambda x: x[1], reverse=True)[0:10]
  words = [w[0] for w in count_dict]
  counts = [w[1] for w in count_dict]
  x_pos = np.arange(len(words))

  plt.figure(2, figsize=(15, 15 / 1.6180))
  plt.subplot(title='10 most common words')
  sns.set_context("notebook", font_scale=1.25, rc={"lines.linewidth": 2.5})
  sns.barplot(x_pos, counts, palette='husl')
  plt.xticks(x_pos, words, rotation=90)
  plt.xlabel('words')
  plt.ylabel('counts')
  plt.show()


# Initialise the count vectorizer with the English stop words
count_vectorizer = CountVectorizer(stop_words='english')
# Fit and transform the processed titles
count_data = count_vectorizer.fit_transform(Finaldata["Abstract_cleaned"])
# Visualise the 10 most common words
plot_10_most_common_words(count_data, count_vectorizer)

#import warnings

#warnings.simplefilter("ignore", DeprecationWarning)
# Load the LDA model from sk-learn
from sklearn.decomposition import LatentDirichletAllocation as LDA
#warnings.warn("ignore", DeprecationWarning)

# Helper function
def print_topics(model, count_vectorizer, n_top_words):
  words = count_vectorizer.get_feature_names()
  for topic_idx, topic in enumerate(model.components_):
    print("\nTopic #%d:" % topic_idx)
    print(" ".join([words[i]
                    for i in topic.argsort()[:-n_top_words - 1:-1]]))


# Tweak the two parameters below
number_topics =200
number_words =50
# Create and fit the LDA model
lda = LDA(n_components=number_topics, n_jobs=-1)
lda.fit(count_data)
# Print the topics found by the LDA model
print("Topics found via LDA:")
print_topics(lda, count_vectorizer, number_words)


